{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5b392f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\geeth\\anaconda3\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in c:\\users\\geeth\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2021.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\geeth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\geeth\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\geeth\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.25.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.6.5)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.3.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2021.10.1)\n",
      "Requirement already satisfied: requests in c:\\users\\geeth\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.9)\n",
      "Requirement already satisfied: networkx in c:\\users\\geeth\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.6.0->sentence-transformers) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.8.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: click in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (8.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\geeth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\geeth\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: truecase in c:\\users\\geeth\\anaconda3\\lib\\site-packages (0.0.14)\n",
      "Requirement already satisfied: nltk in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from truecase) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from nltk->truecase) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from nltk->truecase) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from nltk->truecase) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from nltk->truecase) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from click->nltk->truecase) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\geeth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\geeth\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\geeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\geeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\geeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\geeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py2neo in c:\\users\\geeth\\anaconda3\\lib\\site-packages (2021.2.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from py2neo) (2021.10.8)\n",
      "Requirement already satisfied: interchange~=2021.0.4 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from py2neo) (2021.0.4)\n",
      "Requirement already satisfied: monotonic in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from py2neo) (1.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from py2neo) (23.1)\n",
      "Requirement already satisfied: pansi>=2020.7.3 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from py2neo) (2020.7.3)\n",
      "Requirement already satisfied: pygments>=2.0.0 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from py2neo) (2.15.1)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from py2neo) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from py2neo) (1.26.7)\n",
      "Requirement already satisfied: pytz in c:\\users\\geeth\\anaconda3\\lib\\site-packages (from interchange~=2021.0.4->py2neo) (2021.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\geeth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\geeth\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "from neo4j import GraphDatabase\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers\n",
    "!pip3 install truecase\n",
    "\n",
    "\n",
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, manifold\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "\n",
    "## for w2v\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "## for bert\n",
    "import transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import truecase\n",
    "\n",
    "!pip install py2neo\n",
    "from py2neo import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a9372",
   "metadata": {},
   "source": [
    "#### Dictionary Cluster for various categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf677752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISEASE :  ['disease', 'condition', 'illness', 'condition', 'sickness'] ... 5 \n",
      "\n",
      "DRUGS :  ['drug', 'medication', 'medicine', 'pills', 'remedy'] ... 5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dic_clusters = {}\n",
    "dic_clusters[\"DISEASE\"] = ['disease', 'condition', 'illness', 'condition', 'sickness']\n",
    "dic_clusters[\"DRUGS\"] = ['drug', 'medication', 'medicine', 'pills', 'remedy']\n",
    "   \n",
    "    \n",
    "#Removing Punctuation, Null entries and stopwords\n",
    "\n",
    "def Cleaning_1_1(dic_clusters):\n",
    "\n",
    "  lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "  for k,v in dic_clusters.items():\n",
    "    for word in v:\n",
    "      if len(word)<=2:\n",
    "        v.remove(word)\n",
    "\n",
    "    for i in lst_stopwords:\n",
    "      if i in v:\n",
    "        v.remove(i)\n",
    "\n",
    "    new_list = v\n",
    "    dic_clusters.update({k: list(filter(None, new_list))})\n",
    "\n",
    "  return dic_clusters   \n",
    "\n",
    "#Removing duplicates between the dictionaries\n",
    "\n",
    "def Cleaning_1_2(dic_clusters):\n",
    "  seen = set()\n",
    "  repeated = set()\n",
    "  new_list=[]\n",
    "\n",
    "  for k,v in dic_clusters.items():\n",
    "    for i in set(v):\n",
    "      if i in seen:\n",
    "        repeated.add(i)\n",
    "      else:\n",
    "        seen.add(i) \n",
    "    \n",
    "  for i in repeated:\n",
    "    x=0\n",
    "    for k,v in dic_clusters.items():\n",
    "      new_list2 = v\n",
    "      if i in v:\n",
    "        x=x+1\n",
    "        if x>1:\n",
    "          new_list2.remove(i)\n",
    "     \n",
    "      dic_clusters.update({k: new_list2})\n",
    "\n",
    "Cleaning_1_1(dic_clusters)\n",
    "Cleaning_1_2(dic_clusters)\n",
    "\n",
    "#Cleaned Dictionary cluster\n",
    "\n",
    "for k,v in dic_clusters.items():\n",
    "    print(k, \": \", v[0:len(v)], \"...\", len(v),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a375d",
   "metadata": {},
   "source": [
    "#### Question Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637f8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and cleaning the question\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    \n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6981b7f",
   "metadata": {},
   "source": [
    "#### Generates BERT embeddings for cleaned question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51231401",
   "metadata": {},
   "source": [
    "The 'utils_bert_embedding' function utilizes the BERT tokenizer and model to encode the input text, extract the BERT embeddings, and return them as a NumPy array. This function is helpful for obtaining contextualized word representations using the BERT model, which can be used for various downstream tasks such as text classification, named entity recognition, or question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55b4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_bert_embedding(txt, tokenizer, nlp):\n",
    "    idx = tokenizer.encode(txt)\n",
    "    idx = np.array(idx)[None,:]  \n",
    "    embedding = nlp(idx)\n",
    "    X = np.array(embedding[0][0][1:-1])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7339edd",
   "metadata": {},
   "source": [
    "#### Cleaning Questions in Templates CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0724cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and cleaning method 2\n",
    "\n",
    "'''\n",
    "Preprocess a string.\n",
    ":parameter\n",
    "    :param text: string - name of column containing text\n",
    "    :param lst_stopwords: list - list of stopwords to remove\n",
    "    :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    ":return\n",
    "    cleaned text\n",
    "'''\n",
    "def utils_preprocess_text2(text, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    \n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7a72c",
   "metadata": {},
   "source": [
    "#### Linguistic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6442fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_LinguisticApproach(question, templates):\n",
    "  score = []\n",
    "  for template in templates[\"Template_Clean3\"]:\n",
    "    sc=0\n",
    "    for word in question.split():\n",
    "      if word in template.split():\n",
    "        sc=sc+1\n",
    "    score.append(sc)\n",
    "\n",
    "  return score    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90303f36",
   "metadata": {},
   "source": [
    "#### Cypher Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20fc46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_Retrieval(Final_predictions, Question, templates,subject,selected_cat,qtype,predicate):\n",
    "    print(\"cypher_query\")\n",
    "    for p in Final_predictions:\n",
    "        #fetching the appropriate template\n",
    "        template_index = int(p[1])\n",
    "        #print(templates[\"Cypher_Clean\"])\n",
    "        #print(template_index)\n",
    "        cypher_query = templates[\"Cypher_Clean\"].iloc[template_index]\n",
    "        query = cypher_query.replace('{first_item_name_}', subject)\n",
    "        print(query)\n",
    "        #print(selected_cat)\n",
    "        #print(qtype)\n",
    "        #print(predicate)\n",
    "        if (qtype == 'true'):\n",
    "            query = query.replace('{second_item_name_}', selected_cat)\n",
    "            query = query.replace('{reltn_}', predicate)\n",
    "        print(query)\n",
    "    return query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def46a92",
   "metadata": {},
   "source": [
    "### Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "438af3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_generation(question,obj_text,selected_cat,qtype,predicate):\n",
    "\tprint(\"Question ---\",question)\n",
    "\t#Question = \"Are there any medical conditions that often lead to hepatic failure\"\n",
    "\tQuestion = question\n",
    "\tlst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\t#Question preparation\n",
    "\tClean_q = utils_preprocess_text(Question, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords)\n",
    "\tprint(\"Output of Question preparation ---\",Clean_q)\n",
    " \n",
    "          \n",
    "          \n",
    "\t#Context Identification      \n",
    "\ttokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\tnlp = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\tlst_mean_vecs = utils_bert_embedding(Clean_q, tokenizer, nlp).mean(0)  #generates BERT embeddings and calculate the mean vector of these embeddings\n",
    "\tX = np.array(lst_mean_vecs) #Convert the list of mean vectors (lst_mean_vecs) to a NumPy array (X).\n",
    "    \n",
    "\t#dic_y --> keys = items of the dic_clusters,  --> values = mean vectors calculated using the utils_bert_embedding function \n",
    "\tdic_y = {k:utils_bert_embedding(v, tokenizer, nlp).mean(0) for k,v\n",
    "         in dic_clusters.items()}\n",
    "\t## compute cosine similarities\n",
    "\tsimilarities_q = np.array(\n",
    "\t\t\t\t[metrics.pairwise.cosine_similarity(X.reshape(1, -1), y.reshape(1, -1)).T.tolist()[0] \n",
    "\t\t\t\t for y in dic_y.values()]\n",
    "\t\t\t\t).T\n",
    "\t## adjust and rescale\n",
    "\tlabels = list(dic_y.keys())\n",
    "\tprint(similarities_q)\n",
    "\tprint(labels)    \n",
    "    \n",
    "\t#Even when there is no similarity, there is still a label assigned and the resulting probabilities sum up to 1 for each row.\n",
    "\tfor i in range(len(similarities_q)):\n",
    "\t\t### assign randomly if there is no similarity\n",
    "\t\tif sum(similarities_q[i]) == 0:\n",
    "\t\t   similarities_q[i] = [0]*len(labels)\n",
    "\t\t   similarities_q[i][np.random.choice(range(len(labels)))] = 1\n",
    "\t\t### rescale so they sum = 1\n",
    "\t\tsimilarities_q[i] = similarities_q[i] / sum(similarities_q[i])\n",
    "\t\tprint(similarities_q[i])    \n",
    "\t## classify the label with highest similarity score\n",
    "\tpredicted_prob_q = similarities_q\n",
    "\tpredicted_q = [labels[np.argmax(pred)] for pred in predicted_prob_q]\n",
    "\tprint('Predicted Context :: ',predicted_q)\n",
    "\n",
    "          \n",
    "          \n",
    "\t# Template Matching\n",
    "\ttemplates = pd.read_csv(predicted_q[0] + \".csv\", sep=\",\", engine=\"python\")\n",
    "\t#Cleaning Templates with utils_preprocess_text2 \n",
    "\tlst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\tindex=0\n",
    "\tfor sentence in templates[\"Template\"]:\n",
    "\t\t templates.at[index,'Template_Clean3'] = utils_preprocess_text2(sentence, flg_lemm=True)\n",
    "\t\t index=index+1\n",
    "\t#Cleaning Question with utils_preprocess_text2          \n",
    "\tQuestion_simil_clean= utils_preprocess_text2(Question, flg_lemm=True)\n",
    "\tmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2') #The model is responsible for generating sentence embeddings, which are vector representations of sentences that capture their semantic and contextual information.\n",
    "\ttemplate_embeddings = model.encode(templates[\"Template_Clean3\"])\n",
    "\tquestion_embeddings = model.encode(Question_simil_clean)\n",
    "\t#for every question we calculate how similar it is to each template          \n",
    "\tsimilarities2 = cosine_similarity(question_embeddings.reshape(1, -1), template_embeddings)\n",
    "\tfor i in range(len(similarities2)):\n",
    "\t\t### rescale so they sum = 1\n",
    "\t\tsimilarities2[i] = similarities2[i] / sum(similarities2[i])\n",
    "\t## classify the label with highest similarity score\n",
    "\tpredicted_prob = similarities2\n",
    "\tpredicted = []\n",
    "\tsecond =[]\n",
    "\n",
    "\tfor pred in predicted_prob:\n",
    "\t  x = np.argmax(pred)\n",
    "\t  predicted.append(\"T\" + str(x))\n",
    "\t  #getting the second biggest similarity\n",
    "\t  temp = np.copy(pred)\n",
    "\t  temp[x] = -1\n",
    "\t  x2=np.argmax(temp)\n",
    "\t  second.append(\"T\" + str(x2))\n",
    "\tindex=0\n",
    "\tfor sentence in templates[\"Template\"]:\n",
    "\t  templates.at[index,'Template_Clean3'] = utils_preprocess_text2(sentence, flg_lemm=True, lst_stopwords=lst_stopwords)\n",
    "\t  index=index+1\n",
    "\tQuestion_ling_clean = utils_preprocess_text2(Question, flg_lemm=True, lst_stopwords=lst_stopwords)\n",
    "\tscores = []\n",
    "\tscores = Similarity_LinguisticApproach(Question_ling_clean, templates)\n",
    "\tcount=0\n",
    "\tfor i in scores:\n",
    "\t  count=count+i\n",
    "\tif count==0:\n",
    "\t\tscores.clear()\n",
    "\tpredicted_prob = scores\n",
    "\tpredicted2=[]\n",
    "\tif predicted_prob:\n",
    "\t  x = np.argmax(pred)\n",
    "\t  predicted2.append(\"T\" + str(x)) \n",
    "\telse:\n",
    "\t  predicted2.append(None)\n",
    "\tFinal_predictions=[]\n",
    "\n",
    "\ti=0\n",
    "\tfor p1, p2 in zip(predicted, predicted2):\n",
    "\t\tif p2:\n",
    "\t\t\tif p2 == p1 :\n",
    "\t\t\t\tfinal_Pred = p1\n",
    "\t\t\telif p2==second[i]:\n",
    "\t\t\t\tfinal_Pred = second[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tfinal_Pred = p2\n",
    "\t\telse:\n",
    "\t\t\tfinal_Pred = p1 \n",
    "\t\ti=i+1\n",
    "\t\tFinal_predictions.append(final_Pred)\n",
    "\tprint(\"Final Predictions\", Final_predictions)\n",
    "\n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "\t# Query Retrieval\n",
    "\t\n",
    "\tindex=0\n",
    "\tfor cyph_q in templates[\"Cypher\"]:\n",
    "\t  new_cyph_q = re.sub(\"<+[\\w]*>\", \"#\", cyph_q)\n",
    "\t  templates.at[index,'Cypher_Clean'] = new_cyph_q\n",
    "\t  index=index+1\n",
    "\t  \n",
    "\ttemplates[\"Cypher_Clean\"].head()\n",
    "\tprint(\"\\nFinal_predictions: \", Final_predictions)\n",
    "\tquery = answer_Retrieval(Final_predictions, Question, templates,obj_text,selected_cat,qtype,predicate)\n",
    "\tprint(\"Cypher Query: \",query)\n",
    "\n",
    "\n",
    "\treturn query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3b428",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8080/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [24/Aug/2023 08:57:20] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question --- Which diseases are commonly associated with Anemia\n",
      "Output of Question preparation --- disease commonly associated anemia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52727485 0.461806  ]]\n",
      "['DISEASE', 'DRUGS']\n",
      "[0.5330958 0.4669042]\n",
      "Predicted Context ::  ['DISEASE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Aug/2023 09:07:04] \"POST /convert_nlp_to_cypher HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Predictions ['T1']\n",
      "\n",
      "Final_predictions:  ['T1']\n",
      "cypher_query\n",
      "MATCH (e:First_Item)-[r:Relation]->(c:Second_Item) WHERE c.Name =~ 'Anemia' AND NOT (e)-[:Relation {type: 'belongs to the drug family of'}]->() OPTIONAL MATCH (e)<-[r_inv:InverseRelation]-(c) RETURN e, r, c, r_inv\n",
      "MATCH (e:First_Item)-[r:Relation]->(c:Second_Item) WHERE c.Name =~ 'Anemia' AND NOT (e)-[:Relation {type: 'belongs to the drug family of'}]->() OPTIONAL MATCH (e)<-[r_inv:InverseRelation]-(c) RETURN e, r, c, r_inv\n",
      "Cypher Query:  MATCH (e:First_Item)-[r:Relation]->(c:Second_Item) WHERE c.Name =~ 'Anemia' AND NOT (e)-[:Relation {type: 'belongs to the drug family of'}]->() OPTIONAL MATCH (e)<-[r_inv:InverseRelation]-(c) RETURN e, r, c, r_inv\n",
      "Question --- What Thiazolidinediones may cause Anemia\n",
      "Output of Question preparation --- thiazolidinediones may cause anemia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32602039 0.43615019]]\n",
      "['DISEASE', 'DRUGS']\n",
      "[0.42775252 0.57224748]\n",
      "Predicted Context ::  ['DRUGS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Aug/2023 09:07:59] \"POST /convert_nlp_to_cypher HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Predictions ['T4']\n",
      "\n",
      "Final_predictions:  ['T4']\n",
      "cypher_query\n",
      "MATCH (e:First_Item)-[r:Relation{type: '{reltn_}'}]->(c:Second_Item) WHERE c.Name =~ 'Anemia' MATCH (e)-[dr:Relation]->(f) WHERE f.Name =~ '{second_item_name_}' RETURN e, r, c, dr, f\n",
      "MATCH (e:First_Item)-[r:Relation{type: 'may cause'}]->(c:Second_Item) WHERE c.Name =~ 'Anemia' MATCH (e)-[dr:Relation]->(f) WHERE f.Name =~ 'Thiazolidinediones' RETURN e, r, c, dr, f\n",
      "Cypher Query:  MATCH (e:First_Item)-[r:Relation{type: 'may cause'}]->(c:Second_Item) WHERE c.Name =~ 'Anemia' MATCH (e)-[dr:Relation]->(f) WHERE f.Name =~ 'Thiazolidinediones' RETURN e, r, c, dr, f\n",
      "Question --- Which symptoms are commonly associated with Anemia\n",
      "Output of Question preparation --- symptom commonly associated anemia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question --- Which symptoms are commonly associated with Anemia\n",
      "Output of Question preparation --- symptom commonly associated anemia\n",
      "[[0.45459807 0.4224146 ]]\n",
      "['DISEASE', 'DRUGS']\n",
      "[0.51834835 0.48165165]\n",
      "Predicted Context ::  ['DISEASE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Aug/2023 09:09:37] \"POST /convert_nlp_to_cypher HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Predictions ['T6']\n",
      "\n",
      "Final_predictions:  ['T6']\n",
      "cypher_query\n",
      "MATCH (e:First_Item)-[r:Relation]->(c:Second_Item) WHERE e.Name =~ 'Anemia' AND NOT (e)-[:Relation {type: 'belongs to the drug family of'}]->() OPTIONAL MATCH (e)<-[r_inv:InverseRelation]-(c) RETURN e, r, c, r_inv\n",
      "MATCH (e:First_Item)-[r:Relation]->(c:Second_Item) WHERE e.Name =~ 'Anemia' AND NOT (e)-[:Relation {type: 'belongs to the drug family of'}]->() OPTIONAL MATCH (e)<-[r_inv:InverseRelation]-(c) RETURN e, r, c, r_inv\n",
      "Cypher Query:  MATCH (e:First_Item)-[r:Relation]->(c:Second_Item) WHERE e.Name =~ 'Anemia' AND NOT (e)-[:Relation {type: 'belongs to the drug family of'}]->() OPTIONAL MATCH (e)<-[r_inv:InverseRelation]-(c) RETURN e, r, c, r_inv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45459807 0.4224146 ]]\n",
      "['DISEASE', 'DRUGS']\n",
      "[0.51834835 0.48165165]\n",
      "Predicted Context ::  ['DISEASE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Aug/2023 09:09:39] \"POST /convert_nlp_to_cypher HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Predictions ['T6']\n",
      "\n",
      "Final_predictions:  ['T6']\n",
      "cypher_query\n",
      "MATCH (e:First_Item)-[r:Relation]->(c:Second_Item) WHERE e.Name =~ 'Anemia' AND NOT (e)-[:Relation {type: 'belongs to the drug family of'}]->() OPTIONAL MATCH (e)<-[r_inv:InverseRelation]-(c) RETURN e, r, c, r_inv\n",
      "MATCH (e:First_Item)-[r:Relation]->(c:Second_Item) WHERE e.Name =~ 'Anemia' AND NOT (e)-[:Relation {type: 'belongs to the drug family of'}]->() OPTIONAL MATCH (e)<-[r_inv:InverseRelation]-(c) RETURN e, r, c, r_inv\n",
      "Cypher Query:  MATCH (e:First_Item)-[r:Relation]->(c:Second_Item) WHERE e.Name =~ 'Anemia' AND NOT (e)-[:Relation {type: 'belongs to the drug family of'}]->() OPTIONAL MATCH (e)<-[r_inv:InverseRelation]-(c) RETURN e, r, c, r_inv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the route for the home page\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "\n",
    "# Define the route for the Cypher query - Student\n",
    "@app.route('/convert_nlp_to_cypher', methods=['POST'])\n",
    "def stu_query():\n",
    "    # Get the natural language query from the HTML form\n",
    "    query_text = request.form.get('nlp_query')\n",
    "    obj_text = request.form.get('obj_text')\n",
    "    qtype = request.form.get('qtype')\n",
    "    selected_cat = request.form.get('selected_cat')\n",
    "    predicate = request.form.get('predicate')\n",
    "    cy_query = query_generation(query_text,obj_text,selected_cat,qtype,predicate)\n",
    "    return cy_query\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8080)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df4b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
